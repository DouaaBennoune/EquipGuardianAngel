{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc110aaf",
   "metadata": {},
   "source": [
    "# **Dataset Preprocessing**\n",
    "This notebook demonstrates the complete pipeline for:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960028b",
   "metadata": {},
   "source": [
    "## **1-Cleaning** \n",
    "### üìå Step 1: Define Sensor Groups\n",
    " We start by defining two sets of sensors based on prior research:\n",
    " - **Known Constant Sensors** These sensors provide no useful variation and are droppedsensor_20, sensor_21)\n",
    "\n",
    "- **Critical Sensors for RUL Prediction**  \n",
    "These are operating conditions and sensors proven to be predictive of RUL:\n",
    "- Operating conditions: `setting1`, `setting2`, `setting3`\n",
    "- Temperatures: `sensor_2`, `sensor_3`, `sensor_4`\n",
    "- Pressure ratios: `sensor_7`, `sensor_8`, `sensor_9`\n",
    "- HPC/LPT: `sensor_11`, `sensor_12`, `sensor_13`\n",
    "- Flow ratios: `sensor_14`, `sensor_15`\n",
    "- Coolant/Bleed: `sensor_17`, `sensor_20`, `sensor_21`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051a8ac",
   "metadata": {},
   "source": [
    "## üìå Step 2: Cleaning Function\n",
    "\n",
    "We define `clean_train_df(filepath)` to process each training dataset:\n",
    "\n",
    "1. **Load raw data**  \n",
    " - Reads space-separated text files.  \n",
    " - Assigns column names: `id`, `cycle`, 3 settings, and 21 sensors.\n",
    "\n",
    "2. **Compute RUL**  \n",
    " - For each engine (`id`),  \n",
    "   \n",
    "\n",
    "\\[\n",
    "   RUL = \\max(\\text{cycle}) - \\text{cycle}\n",
    "   \\]\n",
    "\n",
    "\n",
    "\n",
    "3. **Drop constant sensors**  \n",
    " - Removes the sensors listed in `CONSTANT_SENSORS`.\n",
    "\n",
    "4. **Normalize operating regimes**  \n",
    " - Discretizes `setting1` into bins (`op_regime`) to capture different operating conditions.\n",
    "\n",
    "5. **Retain critical features**  \n",
    " - Keeps only `id`, `cycle`, `RUL`, and the critical sensors.\n",
    "\n",
    "6. **Correlation filtering**  \n",
    " - Removes one sensor from pairs with correlation > 0.9, keeping the one more correlated with RUL.\n",
    "\n",
    "7. **Variance check**  \n",
    " - Drops sensors with zero variance.\n",
    "\n",
    "8. **Return**  \n",
    " - Cleaned DataFrame.  \n",
    " - Separate RUL series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3f7c1",
   "metadata": {},
   "source": [
    "## üìå Step 3: Process All Training Sets\n",
    "\n",
    "We process the four CMAPSS training datasets:\n",
    "\n",
    "```python\n",
    "trains_path = [\n",
    "  '.../train_FD001.txt',\n",
    "  '.../train_FD002.txt',\n",
    "  '.../train_FD003.txt',\n",
    "  '.../train_FD004.txt'\n",
    "]\n",
    "\n",
    "train_datasets = [clean_train_df(f) for f in trains_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d2bf3",
   "metadata": {},
   "source": [
    "## Install Dpendencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b44401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install  pandas numpy matplolib scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f975eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48514ab",
   "metadata": {},
   "source": [
    "# # - Place your raw .txt datasets inside the \"dataset\" folder\n",
    "# you find them in the datasets folder in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5fabc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURATION (User Editable)\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path(os.getcwd())   # fallback for Jupyter\n",
    "\n",
    "DATASET_DIR = BASE_DIR / \"dataset\"\n",
    "OUTPUT_DIR = BASE_DIR / \"CLEANED_DATA\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TRAIN_DIR = OUTPUT_DIR / \"train\"\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_DIR = OUTPUT_DIR / \"test\"\n",
    "TEST_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb48773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known constant sensors in C-MAPSS (from literature)\n",
    "CONSTANT_SENSORS = ['sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', \n",
    "                    'sensor_16', 'sensor_18', 'sensor_19']\n",
    "\n",
    "# Critical sensors for RUL prediction (proven in research)\n",
    "CRITICAL_SENSORS = [\n",
    "    'setting1', 'setting2', 'setting3',  # Operating conditions\n",
    "    'sensor_2', 'sensor_3', 'sensor_4',   # Temperatures\n",
    "    'sensor_7', 'sensor_8', 'sensor_9',   # Pressure ratios\n",
    "    'sensor_11', 'sensor_12', 'sensor_13', # HPC/LPT\n",
    "    'sensor_14', 'sensor_15',              # Flow ratios\n",
    "    'sensor_17', 'sensor_20', 'sensor_21'  # Coolant/bleed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb7790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_df(filepath):\n",
    "    df = pd.read_csv(filepath, sep=r'\\s+', header=None)\n",
    "    \n",
    "    # Define columns\n",
    "    column_names = [\"id\", \"cycle\", \"setting1\", \"setting2\", \"setting3\"] + \\\n",
    "                   [f\"sensor_{i}\" for i in range(1, 22)]\n",
    "    df.columns = column_names\n",
    "    df = df.copy()\n",
    "    #  Compute RUL\n",
    "    df['RUL'] = df.groupby('id')['cycle'].transform('max') - df['cycle']\n",
    "    # Drop constant sensors\n",
    "    df = df.drop(columns=CONSTANT_SENSORS, errors=\"ignore\") \n",
    "    # Retain critical features \n",
    "    df['op_regime'] = pd.cut(df['setting1'], bins=5, labels=False)\n",
    "    sensor_cols = [c for c in df.columns if c.startswith('sensor_')]\n",
    "    for sensor in sensor_cols:\n",
    "        # Normalize by operating regime mean/std\n",
    "        df[f'{sensor}_norm'] = df.groupby('op_regime')[sensor].transform(\n",
    "            lambda x: (x - x.mean()) / (x.std() + 1e-8)\n",
    "        )\n",
    "    \n",
    "    keep_cols = ['id', 'cycle', 'RUL'] + \\\n",
    "                [c for c in CRITICAL_SENSORS if c in df.columns] + \\\n",
    "                [c for c in df.columns if c.endswith('_norm')]\n",
    "    \n",
    "    df = df[keep_cols] \n",
    "    # Remove highly correlated features (>0.9) \n",
    "    sensor_norm_cols = [c for c in df.columns if c.endswith('_norm')]\n",
    "    corr_matrix = df[sensor_norm_cols].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    to_drop = []\n",
    "    for column in upper.columns:\n",
    "        if any(upper[column] > 0.9):\n",
    "            # Keep the one with higher RUL correlation\n",
    "            corr_with_rul = df[[column] + ['RUL']].corr().iloc[0, 1]\n",
    "            partner = upper[column].idxmax()\n",
    "            partner_corr = df[[partner] + ['RUL']].corr().iloc[0, 1]\n",
    "            \n",
    "            if abs(corr_with_rul) < abs(partner_corr):\n",
    "                to_drop.append(column)\n",
    "    \n",
    "    df = df.drop(columns=to_drop, errors='ignore')\n",
    "    \n",
    "    valid_norm_cols = [c for c in sensor_norm_cols if c in df.columns]\n",
    "    variance = df[valid_norm_cols].var()\n",
    "    \n",
    "    zero_var = variance[variance == 0].index.tolist()\n",
    "    df = df.drop(columns=zero_var, errors='ignore')\n",
    "    df_rul=df['RUL']\n",
    "    return df.copy(),df_rul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002ce526",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [\n",
    "    DATASET_DIR /\"train_FD001.txt\",\n",
    "    DATASET_DIR /\"train_FD002.txt\",\n",
    "    DATASET_DIR /\"train_FD003.txt\",\n",
    "    DATASET_DIR /\"train_FD004.txt\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08e8144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_test_df(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean a single C-MAPSS test dataset.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to raw test file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned test dataset.\n",
    "    \"\"\"\n",
    "    # Read raw data\n",
    "    df = pd.read_csv(filepath, sep=r'\\s+', header=None)\n",
    "    \n",
    "    # Define columns\n",
    "    column_names = [\"id\", \"cycle\", \"setting1\", \"setting2\", \"setting3\"] + \\\n",
    "                   [f\"sensor_{i}\" for i in range(1, 22)]\n",
    "    df.columns = column_names\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.drop(columns=CONSTANT_SENSORS, errors='ignore')\n",
    "    \n",
    "    df['op_regime']=pd.cut(df['setting1'], bins=5, labels=False)\n",
    "\n",
    "    \n",
    "    sensor_cols = [c for c in df.columns if c.startswith('sensor_')]\n",
    "    for sensor in sensor_cols:\n",
    "        # Normalize by operating regime mean/std\n",
    "        df[f'{sensor}_norm'] = df.groupby('op_regime')[sensor].transform(\n",
    "            lambda x: (x - x.mean()) / (x.std() + 1e-8)\n",
    "        )\n",
    "    \n",
    "    # 4Ô∏è‚É£ Use normalized sensors + keep critical features\n",
    "    keep_cols = ['id', 'cycle'] + \\\n",
    "                [c for c in CRITICAL_SENSORS if c in df.columns] + \\\n",
    "                [c for c in df.columns if c.endswith('_norm')]\n",
    "    \n",
    "    df = df[keep_cols]\n",
    "\n",
    "    return df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24ee2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_files = [\n",
    "    DATASET_DIR / \"test_FD001.txt\",\n",
    "    DATASET_DIR / \"test_FD002.txt\",\n",
    "    DATASET_DIR / \"test_FD003.txt\",\n",
    "    DATASET_DIR / \"test_FD004.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a93b3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aligned datasets have been saved to CLEANED_DATA/train and CLEANED_DATA/test\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Align Train/Test Datasets\n",
    "-------------------------\n",
    "Ensures that training and test sets share the same feature columns.\n",
    "Outputs aligned CSVs for each FD dataset into CLEANED_DATA/train and CLEANED_DATA/test.\n",
    "\"\"\"\n",
    "\n",
    "# After cleaning\n",
    "train_datasets = [clean_train_df(f) for f in train_files]   # list of (df_cleaned, df_rul)\n",
    "test_datasets  = [clean_test_df(f) for f in test_files]     # list of df_cleaned\n",
    "\n",
    "# Align\n",
    "aligned_pairs = []\n",
    "for i, ((df_cleaned, df_rul), test_df) in enumerate(zip(train_datasets, test_datasets), start=1):\n",
    "    common_cols = list(set(df_cleaned.columns) & set(test_df.columns))\n",
    "    train_cols = common_cols + [\"RUL\"]\n",
    "\n",
    "    train_aligned = df_cleaned.reindex(columns=train_cols)\n",
    "    test_aligned  = test_df.reindex(columns=common_cols)\n",
    "\n",
    "    train_aligned.to_csv(TRAIN_DIR / f\"train_FD00{i}_aligned.csv\", index=False)\n",
    "    test_aligned.to_csv(TEST_DIR / f\"test_FD00{i}_aligned.csv\", index=False)\n",
    "\n",
    "    aligned_pairs.append((train_aligned, test_aligned))\n",
    "\n",
    "print(\" Aligned datasets have been saved to CLEANED_DATA/train and CLEANED_DATA/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6f952d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FD001 Dataset:\n",
      "Train columns (33): ['RUL', 'cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_20_norm', 'sensor_21', 'sensor_21_norm', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_8_norm', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n",
      "Test columns (32): ['cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_20_norm', 'sensor_21', 'sensor_21_norm', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_8_norm', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n",
      "\n",
      "FD002 Dataset:\n",
      "Train columns (31): ['RUL', 'cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_14_norm', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_21', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n",
      "Test columns (30): ['cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_14_norm', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_21', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n",
      "\n",
      "FD003 Dataset:\n",
      "Train columns (33): ['RUL', 'cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_20_norm', 'sensor_21', 'sensor_21_norm', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_8_norm', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n",
      "Test columns (32): ['cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_20_norm', 'sensor_21', 'sensor_21_norm', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_8_norm', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n",
      "\n",
      "FD004 Dataset:\n",
      "Train columns (32): ['RUL', 'cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_14_norm', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_21', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_8_norm', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n",
      "Test columns (31): ['cycle', 'id', 'sensor_11', 'sensor_11_norm', 'sensor_12', 'sensor_12_norm', 'sensor_13', 'sensor_13_norm', 'sensor_14', 'sensor_14_norm', 'sensor_15', 'sensor_15_norm', 'sensor_17', 'sensor_17_norm', 'sensor_2', 'sensor_20', 'sensor_21', 'sensor_2_norm', 'sensor_3', 'sensor_3_norm', 'sensor_4', 'sensor_4_norm', 'sensor_7', 'sensor_7_norm', 'sensor_8', 'sensor_8_norm', 'sensor_9', 'sensor_9_norm', 'setting1', 'setting2', 'setting3']\n"
     ]
    }
   ],
   "source": [
    "for i, (train_df, test_df) in enumerate(aligned_pairs, 1):\n",
    "    print(f\"\\nFD00{i} Dataset:\")\n",
    "    print(f\"Train columns ({len(train_df.columns)}): {sorted(train_df.columns)}\")\n",
    "    print(f\"Test columns ({len(test_df.columns)}): {sorted(test_df.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
